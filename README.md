# Improving Optical Coherence Tomography Interpretation With Retrieval-Guided Diagnosis and Text Generation

Wendy Lau Ah Wing <sup>1</sup>, Adrian Balajadia<sup>2*</sup>, Alen Thomas<sup>1</sup>, and Maham Mir<sup>1</sup>

<sup>1</sup> Department of Computer Science, University of Calgary, Alberta, Canada

<sup>2</sup>Department of Biomedical Engineering, University of Calgary, Alberta, Canada

<sup>*</sup>Corresponding author: adrian.balajadia@ucalgary.ca (AB)




# Abstract


Optical Coherence Tomography (OCT) is a widely adopted imaging technique in ophthalmology, yet its clinical interpretation remains prone to inconsistency and subjectivity. This study presents a retrieval-augmented generation (RAG) framework aimed at improving both diagnostic accuracy and interpretability in OCT-based analysis. The pipeline integrates a dual-headed convolutional neural network with a case-based retrieval engine and a language model capable of generating structured clinical summaries. A ResNet50 model was trained to classify diabetic retinopathy (DR), diabetic macular edema (DME), and key retinal biomarkers using a public OCT dataset. Feature embeddings generated by the model were indexed into a similarity-search database to enable retrieval of clinically relevant cases. Retrieved scans, along with classification outputs, were used to prompt a large language model to generate concise diagnostic reports that reflect both the modelâ€™s predictions and visually aligned precedents. The system achieved 89% accuracy in distinguishing DR from DME in a hold-out test set. Interpretability was also demonstrated through consistent retrieval of semantically similar cases. Generated reports were found to be readable and aligned with clinical reasoning. By combining classification performance with transparent, case-grounded explanations, the RAG approach enhances clinical confidence and supports decision-making. This work highlights the potential of retrieval-augmented pipelines to reduce variability in OCT interpretation and to serve as a scalable, explainable foundation for future AI-driven diagnostic support in ophthalmology.



## Contents and Usage

### Data Downloading and Data Splitting
Use "Indicium_Download_Data.ipynb" to download the dataset from HuggingFace and split the data as described in the paper. 
Note that the dataset being downloaded is originally from here: https://huggingface.co/datasets/gOLIVES/OLIVES_Dataset

### Training and Running a Classification/Query Example
Use "Indicium_ResNet50_Training_and_Embedding.ipynb" to preprocess the data and run a training loop using the hyperparameters described in the paper. Additionally, this .ipynb file describes how embeddings were saved to a FAISS index and saved for later. After training/loading a model and creating/loading a FAISS index, you can also run a classification or query example. 

### Loading a Model Checkpoint or FAISS Index
This repository also provides the model checkpoints that were used for analysis. Please see the "FAISS Indices" and "Model Checkpoints" folder.  


### Generating a Clinical Report
Use "Indicium_ResNet50_RAG_Demo_and_Case_Evaluations.ipynb" to run a gradio demo to generate a clinical report. Note, you must load a model and FAISS index to use this particular notebook. 
