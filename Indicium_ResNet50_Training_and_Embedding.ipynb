{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adihatake/Improving-OCT-Interpretation-through-Retrieval-Guided-Diagnosis-and-LLM-Based-Reporting/blob/main/Indicium_ResNet50_Training_and_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TpcUDq2lvro"
      },
      "source": [
        "# README!\n",
        "**Cells required to execute:**\n",
        "1. Load the dependencies\n",
        "2. Load datasets from dis\n",
        "3. Preprocess the data\n",
        "4. Define the model\n",
        "\n",
        "**For Inference:**\n",
        "A model must be trained or loaded locally from your disk. Once this has been done, execute the \"inspect an example\" cell\n",
        "\n",
        "**For FAISS Retrieval**\n",
        "A FAISS index must be created or loaded locally from your disk. Execute the \"make a FAISS query\" to make a retrieval.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVKQMv7n5RXC"
      },
      "source": [
        "# Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo0Fg9CRYF0x",
        "outputId": "3448ee3b-5658-4c96-ecb7-a31fc3191e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT8Fz5UyIanc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "mbWXNbjUA1eh",
        "outputId": "50939c80-636d-42e8-9ea6-29f026fdb49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.26 (from langchain-community)\n",
            "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.9/438.9 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.65\n",
            "    Uninstalling langchain-core-0.3.65:\n",
            "      Successfully uninstalled langchain-core-0.3.65\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.25\n",
            "    Uninstalling langchain-0.3.25:\n",
            "      Successfully uninstalled langchain-0.3.25\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.26 langchain-community-0.3.26 langchain-core-0.3.66 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.0 python-dotenv-1.1.0 typing-inspect-0.9.0\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.62 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.66)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.5\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "d608ce8ebc6a451a95ec665373edd67c",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install -U datasets\n",
        "! pip install faiss-cpu\n",
        "! pip install langchain-community\n",
        "! pip install -U langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ghv41nJiv_Rt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import datasets\n",
        "import numpy as np\n",
        "import random\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain.schema import Document\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F82eGeoryGep"
      },
      "source": [
        "# Load datasets from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TJ-ginAppGfC"
      },
      "outputs": [],
      "source": [
        "# Load datasets locally\n",
        "raw_train_data = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/Indicium/OLIVES_train_seed42.parquet\")['train']\n",
        "raw_val_data = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/Indicium/OLIVES_val_seed42.parquet\")['train']\n",
        "raw_test_data = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/Indicium/OLIVES_test_seed42.parquet\")['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wBRrPPOUyVkW"
      },
      "outputs": [],
      "source": [
        "print(raw_train_data)\n",
        "print(raw_val_data)\n",
        "print(raw_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF2qF8QCDikn",
        "outputId": "a687a7b0-61a8-4fde-dbb3-21193766dbff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 14247\n",
            "Test size: 1568\n",
            "Validation size: 1654\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train size: {len(raw_train_data)}\")\n",
        "print(f\"Test size: {len(raw_test_data)}\")\n",
        "print(f\"Validation size: {len(raw_val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qboAqTgiXdQY",
        "outputId": "7a8771f0-32ca-48a7-819b-15e607a74ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train disease label counts:\n",
            "Counter({1.0: 7975, 0.0: 6272})\n",
            "\n",
            "Validation disease label counts:\n",
            "Counter({1.0: 968, 0.0: 686})\n",
            "\n",
            "Test disease label counts:\n",
            "Counter({0.0: 784, 1.0: 784})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get frequency of disease labels in each class\n",
        "train_label_counts = Counter(raw_train_data['Disease Label'])\n",
        "print(\"\\nTrain disease label counts:\")\n",
        "print(train_label_counts)\n",
        "\n",
        "val_label_counts = Counter(raw_val_data['Disease Label'])\n",
        "print(\"\\nValidation disease label counts:\")\n",
        "print(val_label_counts)\n",
        "\n",
        "test_label_counts = Counter(raw_test_data['Disease Label'])\n",
        "print(\"\\nTest disease label counts:\")\n",
        "print(test_label_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCB65s9W_HYg"
      },
      "source": [
        "# Preprocess the data\n",
        "This includes converting the images to RGB and applying ResNet50's standard preprocessing and one-hot encoding the biomarkers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPYpvuZoFDiz"
      },
      "outputs": [],
      "source": [
        "# Define biomarker fields\n",
        "biomarker_fields = [\n",
        "    \"Atrophy / thinning of retinal layers\",\n",
        "    \"Disruption of EZ\",\n",
        "    \"DRIL\",\n",
        "    \"IR hemorrhages\",\n",
        "    \"IR HRF\",\n",
        "    \"Partially attached vitreous face\",\n",
        "    \"Fully attached vitreous face\",\n",
        "    \"Preretinal tissue/hemorrhage\",\n",
        "    \"Vitreous debris\",\n",
        "    \"VMT\",\n",
        "    \"DRT/ME\",\n",
        "    \"Fluid (IRF)\",\n",
        "    \"Fluid (SRF)\",\n",
        "    \"Disruption of RPE\",\n",
        "    \"PED (serous)\",\n",
        "    \"SHRM\"\n",
        "]\n",
        "\n",
        "import io\n",
        "\n",
        "\n",
        "# Define transforms for validation and test datasets\n",
        "resnet_transform = transforms.Compose([\n",
        "    transforms.Resize(256),                        # Resize shorter side to 256\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n",
        "    transforms.ToTensor(),                 # Converts to [C, H, W] in range [0, 1]\n",
        "    transforms.Normalize(                  # Standard ImageNet normalization\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Define transforms for the training data.\n",
        "# Here we use the standard ResNet transform but apply random augmentations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15),  # ±15% brightness and contrast\n",
        "    transforms.RandomHorizontalFlip(p=0.5),                # 50% horizontal flip\n",
        "    transforms.RandomRotation(degrees=8),                  # ±8° rotation\n",
        "    transforms.RandomResizedCrop(224,                      # ResNet expects 224x224\n",
        "                                 scale=(0.25, 1.0),\n",
        "                                 ratio=(0.75, 1.33)),\n",
        "    transforms.Grayscale(num_output_channels=3),           # Convert grayscale to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "class RetinalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, biomarker_fields=None, transform=None):\n",
        "        self.data = data\n",
        "        self.biomarker_fields = biomarker_fields\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_bytes = item['Image']['bytes']\n",
        "        img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        biomarkers = [item[field] for field in self.biomarker_fields]\n",
        "        biomarkers = torch.tensor(biomarkers, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        return {\n",
        "            'image pixel values': img,\n",
        "            'biomarkers': biomarkers,\n",
        "            'label': item['Disease Label'],\n",
        "            'patient_id': item['Patient_ID']\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Define the training and test torch Datasets\n",
        "processed_train_data = RetinalDataset(\n",
        "    data=raw_train_data,\n",
        "    biomarker_fields=biomarker_fields,\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "\n",
        "processed_val_data = RetinalDataset(\n",
        "    data=raw_val_data,\n",
        "    biomarker_fields=biomarker_fields,\n",
        "    transform=resnet_transform\n",
        ")\n",
        "\n",
        "\n",
        "processed_test_data = RetinalDataset(\n",
        "    data=raw_test_data,\n",
        "    biomarker_fields=biomarker_fields,\n",
        "    transform=resnet_transform\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbEeWA-YR1e-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define a manual seeding function to consistently shuffle the datasets\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(42) # Set anual seed to 42 by convention\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Wrap the datasets to a torch DataLoader to generate batches\n",
        "# Here, arrange into batches, shuffle the dataset and use\n",
        "# num_workers to maximize CPU cores for fetching.\n",
        "\n",
        "train_dataloader = DataLoader(processed_train_data,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=4,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "\n",
        "validation_dataloader = DataLoader(processed_val_data,\n",
        "                                   batch_size=BATCH_SIZE,\n",
        "                                   shuffle=False,\n",
        "                                   num_workers=4,\n",
        "                                   worker_init_fn=seed_worker,\n",
        "                                   generator=g)\n",
        "\n",
        "# Used for model testing\n",
        "test_dataloader = DataLoader(processed_test_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuPSiJln20kX"
      },
      "source": [
        "# Define the model\n",
        "Here, we just concatenate the features learned from the biomarker and disease heads and return them for FAISS\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aljw2CSvX22L"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50, ResNet50_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s8qebEqFWsms"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class OCT_net(nn.Module):\n",
        "    def __init__(self, num_biomarkers=len(biomarker_fields), num_diseases=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained ResNet50 and replace FC with identity\n",
        "        pre_trained_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        pre_trained_model.fc = nn.Identity()\n",
        "\n",
        "        # Freeze all layers in the backbone\n",
        "        for param in pre_trained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.backbone = pre_trained_model\n",
        "        self.feature_dim = 2048  # Output of resnet50 with identity FC\n",
        "\n",
        "        # Biomarker regression head\n",
        "        self.biomarker_head = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_biomarkers)\n",
        "        )\n",
        "\n",
        "        # Disease classification head\n",
        "        self.disease_head = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_diseases)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_features=False, head=None):\n",
        "        x = self.backbone(x)  # Shape: (B, 2048)\n",
        "\n",
        "        biomarker_logits = self.biomarker_head(x)\n",
        "        disease_logits = self.disease_head(x)\n",
        "\n",
        "        if return_features:\n",
        "            biomarker_feats = self.biomarker_head[:-1](x)\n",
        "            disease_feats = self.disease_head[:-1](x)\n",
        "            combined = torch.cat([biomarker_feats, disease_feats], dim=1)\n",
        "            embedding = F.normalize(combined, dim=1, p=2)\n",
        "            return biomarker_logits, disease_logits, embedding\n",
        "\n",
        "        if head == 'biomarker':\n",
        "            return biomarker_logits\n",
        "        elif head == 'disease':\n",
        "            return disease_logits\n",
        "\n",
        "        return biomarker_logits, disease_logits\n",
        "\n",
        "\n",
        "model = OCT_net()\n",
        "#print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY1uCfJC5t1Q"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXQvu7qkCm8O"
      },
      "outputs": [],
      "source": [
        "# Check if using GPU\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model = OCT_net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdza2pCvMmiB"
      },
      "outputs": [],
      "source": [
        "# For biomarkers (multi-label classification)\n",
        "biomarker_loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# For disease (multi-class classification)\n",
        "disease_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam as our optimizer\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzTqaanYA-KL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define number of epochs and where to save model checkpoints\n",
        "num_epochs = 10\n",
        "date_entry = \"Jun_23\"\n",
        "checkpoint_dir = f\"/content/drive/MyDrive/Indicium/model_checkpoints/{date_entry}_augmentations_finetuning\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# Define lists to save training and validation metrics.\n",
        "train_losses = []\n",
        "train_bio_losses = []        # biomarker loss for training\n",
        "train_disease_losses = []    # disease loss for training\n",
        "train_accuracies = []\n",
        "\n",
        "val_losses = []\n",
        "val_bio_losses = []          # biomarker loss for validation\n",
        "val_disease_losses = []      # disease loss for validation\n",
        "val_accuracies = []\n",
        "\n",
        "# Describes the main training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Define empty variables to save metrics for later\n",
        "    batch_train_losses = []\n",
        "    batch_train_bio_losses = []\n",
        "    batch_train_disease_losses = []\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # For each batch in the dataset\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "\n",
        "        # Access the images, biomarkers and labels.\n",
        "        # We convert these to the device we are using\n",
        "        # For biomarkers, we convert to float and for disease labels we convert to float 64\n",
        "        images = batch['image pixel values'].to(device)\n",
        "        biomarkers = batch['biomarkers'].float().to(device)\n",
        "        disease_labels = batch['label'].long().to(device)\n",
        "\n",
        "        # Reset the gradients to zero from previous batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the raw scores/logits from the biomarker and disease heades\n",
        "        biomarker_logits, disease_logits = model(images)\n",
        "\n",
        "        # Compute the losses\n",
        "        loss_bio = biomarker_loss_fn(biomarker_logits, biomarkers)\n",
        "        loss_disease = disease_loss_fn(disease_logits, disease_labels)\n",
        "        # Add the losses\n",
        "        total_loss = loss_bio + loss_disease\n",
        "\n",
        "        # Backpropagate the losses\n",
        "        total_loss.backward()\n",
        "        # Update the model's weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save the total loss for this batch\n",
        "        batch_train_losses.append(total_loss.item())\n",
        "        batch_train_bio_losses.append(loss_bio.item())\n",
        "        batch_train_disease_losses.append(loss_disease.item())\n",
        "\n",
        "        # Get predicted class and determine if it is correct.\n",
        "        # Add to the total train and correct train variables\n",
        "        preds = disease_logits.argmax(dim=1)\n",
        "        correct_train += (preds == disease_labels).sum().item()\n",
        "        total_train += disease_labels.size(0)\n",
        "\n",
        "    # Compute the average training loss and accuracies\n",
        "    avg_train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "    avg_train_bio_loss = sum(batch_train_bio_losses) / len(batch_train_bio_losses)\n",
        "    avg_train_disease_loss = sum(batch_train_disease_losses) / len(batch_train_disease_losses)\n",
        "    train_accuracy = correct_train / total_train\n",
        "\n",
        "    # Save these metrics to plot later\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_bio_losses.append(avg_train_bio_loss)\n",
        "    train_disease_losses.append(avg_train_disease_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} (Bio: {avg_train_bio_loss:.4f}, Disease: {avg_train_disease_loss:.4f}), Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval() # Make sure the model is in evaluating mode\n",
        "    batch_val_losses = []\n",
        "    batch_val_bio_losses = []\n",
        "    batch_val_disease_losses = []\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    # Tells model to compute no gradients when evaluating. Very similar to the training loop\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(validation_dataloader, desc=f\"Validating Epoch {epoch+1}\"):\n",
        "            images = batch['image pixel values'].to(device)\n",
        "            biomarkers = batch['biomarkers'].float().to(device)\n",
        "            disease_labels = batch['label'].long().to(device)\n",
        "\n",
        "            biomarker_logits, disease_logits = model(images)\n",
        "\n",
        "            loss_bio = biomarker_loss_fn(biomarker_logits, biomarkers)\n",
        "            loss_disease = disease_loss_fn(disease_logits, disease_labels)\n",
        "            total_loss = loss_bio + loss_disease\n",
        "\n",
        "            batch_val_losses.append(total_loss.item())\n",
        "            batch_val_bio_losses.append(loss_bio.item())\n",
        "            batch_val_disease_losses.append(loss_disease.item())\n",
        "\n",
        "            preds = disease_logits.argmax(dim=1)\n",
        "            correct_val += (preds == disease_labels).sum().item()\n",
        "            total_val += disease_labels.size(0)\n",
        "\n",
        "    # Compute the validation metrics and save for plotting later\n",
        "    avg_val_loss = sum(batch_val_losses) / len(batch_val_losses)\n",
        "    avg_val_bio_loss = sum(batch_val_bio_losses) / len(batch_val_bio_losses)\n",
        "    avg_val_disease_loss = sum(batch_val_disease_losses) / len(batch_val_disease_losses)\n",
        "    val_accuracy = correct_val / total_val\n",
        "\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_bio_losses.append(avg_val_bio_loss)\n",
        "    val_disease_losses.append(avg_val_disease_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Val Loss: {avg_val_loss:.4f} (Bio: {avg_val_bio_loss:.4f}, Disease: {avg_val_disease_loss:.4f}), Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save checkpoint after each epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Specify the epochs, the model's state dictionary (its gradients and weights)\n",
        "    # and other relevant metadata to our pre-defined checkpoint directory\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "\n",
        "        # Metadata\n",
        "        'optimizer_name': type(optimizer).__name__,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'num_epochs': num_epochs,\n",
        "\n",
        "        'train_losses': train_losses,\n",
        "        'train_bio_losses': train_bio_losses,\n",
        "        'train_disease_losses': train_disease_losses,\n",
        "\n",
        "        'val_losses': val_losses,\n",
        "        'val_bio_losses': val_bio_losses,\n",
        "        'val_disease_losses': val_disease_losses,\n",
        "\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'val_accuracies': val_accuracies,\n",
        "    }, checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55O_7GRMAu71"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, num_epochs + 1)\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.8, 1)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7MWt2wzOHbv"
      },
      "source": [
        "# Load trained model from disk\n",
        "Note: you need to run the cell that says \"Define Model\" still."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EsmkWg8qOGjZ"
      },
      "outputs": [],
      "source": [
        "# Check the device to either enable CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/Indicium/model_checkpoints/Jun_21_augmentations_finetuning/model_epoch_3.pth\"  # or whichever checkpoint we want\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# Load the model's state dictionary, weights, gradients and optimizer\n",
        "model = OCT_net()\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# put model to device\n",
        "model.to(device)\n",
        "\n",
        "for param in model.backbone.layer2.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.backbone.layer3.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "for param in model.backbone.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "# Then, reinitialize the optimizer to include all parameters\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5mOkn4RwQZ"
      },
      "source": [
        "Get losses from the transfer learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RiGX6AEWRvsF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Specify the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Define lists for plotting later\n",
        "loaded_train_losses = []\n",
        "loaded_val_losses = []\n",
        "loaded_train_accuracies = []\n",
        "loaded_val_accuracies = []\n",
        "\n",
        "transfer_learning_epochs = 5  # Adjust based on checkpoints\n",
        "\n",
        "for epoch in range(1, transfer_learning_epochs + 1):\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/Indicium/model_checkpoints/Jun_21_augmentations_transferlearning/model_epoch_{epoch}.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Save the losses for plotting later\n",
        "    epoch_train_losses = checkpoint.get('train_losses', [])\n",
        "    epoch_val_losses = checkpoint.get('val_losses', [])\n",
        "    epoch_train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "    epoch_val_accuracies = checkpoint.get('val_accuracies', [])\n",
        "\n",
        "    loaded_train_losses.append(epoch_train_losses[epoch - 1])\n",
        "    loaded_val_losses.append(epoch_val_losses[epoch - 1])\n",
        "    loaded_train_accuracies.append(epoch_train_accuracies[epoch - 1])\n",
        "    loaded_val_accuracies.append(epoch_val_accuracies[epoch - 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYE8c2AodJFx"
      },
      "source": [
        "Get losses from the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-mqUFb40dIbS"
      },
      "outputs": [],
      "source": [
        "fine_tuning_epochs = 10  # Adjust based on checkpoints\n",
        "\n",
        "for epoch in range(1, fine_tuning_epochs + 1):\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/Indicium/model_checkpoints/Jun_23_augmentations_finetuning/model_epoch_{epoch}.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    # Save the losses for plotting later\n",
        "    epoch_train_losses = checkpoint.get('train_losses', [])\n",
        "    epoch_val_losses = checkpoint.get('val_losses', [])\n",
        "    epoch_train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "    epoch_val_accuracies = checkpoint.get('val_accuracies', [])\n",
        "\n",
        "    loaded_train_losses.append(epoch_train_losses[epoch - 1])\n",
        "    loaded_val_losses.append(epoch_val_losses[epoch - 1])\n",
        "    loaded_train_accuracies.append(epoch_train_accuracies[epoch - 1])\n",
        "    loaded_val_accuracies.append(epoch_val_accuracies[epoch - 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSwIOKc7dA0z"
      },
      "outputs": [],
      "source": [
        "epochs_to_plot = range(1, transfer_learning_epochs + fine_tuning_epochs + 1)\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_to_plot, loaded_train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(epochs_to_plot, loaded_val_losses, 'r-', label='Validation Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0.3, 1)\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_to_plot, loaded_train_accuracies, 'b-', label='Training Accuracy')\n",
        "plt.plot(epochs_to_plot, loaded_val_accuracies, 'r-', label='Validation Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.7, 1)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH6sa3m5DU81"
      },
      "source": [
        "# Inspect and run inference on an example:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq8iWHHrEpb0",
        "outputId": "b5124957-a7a1-40b7-9370-9f3c92b9a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random example index: 940\n",
            "True Disease Label: DME\n",
            "Predicted Disease Label: DR\n",
            "Disease Class Probabilities:\n",
            "  DR    : 52.23%\n",
            "  DME   : 47.77%\n",
            "\n",
            "Biomarker Probabilities:\n",
            "Atrophy / thinning of retinal layers | Predicted Probability: 0.502 | True Value: 0.000\n",
            "Disruption of EZ                    | Predicted Probability: 0.490 | True Value: 0.000\n",
            "DRIL                                | Predicted Probability: 0.492 | True Value: 0.000\n",
            "IR hemorrhages                      | Predicted Probability: 0.491 | True Value: 0.000\n",
            "IR HRF                              | Predicted Probability: 0.475 | True Value: 1.000\n",
            "Partially attached vitreous face    | Predicted Probability: 0.512 | True Value: 0.000\n",
            "Fully attached vitreous face        | Predicted Probability: 0.500 | True Value: 1.000\n",
            "Preretinal tissue/hemorrhage        | Predicted Probability: 0.509 | True Value: 0.000\n",
            "Vitreous debris                     | Predicted Probability: 0.502 | True Value: 1.000\n",
            "VMT                                 | Predicted Probability: 0.496 | True Value: 0.000\n",
            "DRT/ME                              | Predicted Probability: 0.520 | True Value: 1.000\n",
            "Fluid (IRF)                         | Predicted Probability: 0.501 | True Value: 1.000\n",
            "Fluid (SRF)                         | Predicted Probability: 0.506 | True Value: 0.000\n",
            "Disruption of RPE                   | Predicted Probability: 0.527 | True Value: 0.000\n",
            "PED (serous)                        | Predicted Probability: 0.499 | True Value: 0.000\n",
            "SHRM                                | Predicted Probability: 0.507 | True Value: 0.000\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# Define the biomarkers and label for indexing\n",
        "label_map = {0: \"DR\", 1: \"DME\"}\n",
        "biomarker_fields = [\n",
        "    \"Atrophy / thinning of retinal layers\",\n",
        "    \"Disruption of EZ\",\n",
        "    \"DRIL\",\n",
        "    \"IR hemorrhages\",\n",
        "    \"IR HRF\",\n",
        "    \"Partially attached vitreous face\",\n",
        "    \"Fully attached vitreous face\",\n",
        "    \"Preretinal tissue/hemorrhage\",\n",
        "    \"Vitreous debris\",\n",
        "    \"VMT\",\n",
        "    \"DRT/ME\",\n",
        "    \"Fluid (IRF)\",\n",
        "    \"Fluid (SRF)\",\n",
        "    \"Disruption of RPE\",\n",
        "    \"PED (serous)\",\n",
        "    \"SHRM\"\n",
        "]\n",
        "\n",
        "# Directly access the dataset\n",
        "dataset = test_dataloader.dataset\n",
        "\n",
        "# Sample random index\n",
        "random_idx = random.randint(0, len(dataset) - 1)\n",
        "\n",
        "# Get a single example from dataset\n",
        "example = dataset[random_idx]\n",
        "\n",
        "# Get the image, biomarkers and labels. Use \"unsqueeze\" since the model expect a batch size of at least 1.\n",
        "image = example['image pixel values'].unsqueeze(0).to(device)\n",
        "true_biomarkers = example['biomarkers'].unsqueeze(0).to(device)\n",
        "true_label = torch.tensor(example['label']).long().unsqueeze(0).to(device)\n",
        "\n",
        "# Convert label if it's not a tensor\n",
        "if not isinstance(example['label'], torch.Tensor):\n",
        "    true_label = torch.tensor(example['label']).long().unsqueeze(0).to(device)\n",
        "else:\n",
        "    true_label = example['label'].long().unsqueeze(0).to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# For inference, do not calculate gradients\n",
        "with torch.no_grad():\n",
        "  # Get the biomarker and disease label probabilities and the embedding\n",
        "    biomarker_logits, disease_logits, learned_features = model(image, return_features=True)\n",
        "\n",
        "# Apply sigmoid for the biomarkers since it is a multi-label classification task\n",
        "biomarker_probs = torch.sigmoid(biomarker_logits).squeeze().cpu().numpy()\n",
        "# Apply softmax for the disease labels, since it is a multi-class classification task\n",
        "disease_probs = F.softmax(disease_logits, dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "# Get the predicted label and the true label\n",
        "predicted_label = disease_probs.argmax()\n",
        "true_label = true_label.item()\n",
        "\n",
        "# Print out the formatted results\n",
        "print(f\"Random example index: {random_idx}\")\n",
        "print(f\"True Disease Label: {label_map[true_label]}\")\n",
        "print(f\"Predicted Disease Label: {label_map[predicted_label]}\")\n",
        "print(\"Disease Class Probabilities:\")\n",
        "for cls_idx, prob in enumerate(disease_probs):\n",
        "    print(f\"  {label_map[cls_idx]:<6}: {prob * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nBiomarker Probabilities:\")\n",
        "for name, prob, true_val in zip(biomarker_fields, biomarker_probs, true_biomarkers.squeeze().cpu().numpy()):\n",
        "    print(f\"{name:<35} | Predicted Probability: {prob:.3f} | True Value: {true_val:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfmwgwZPQEsp"
      },
      "source": [
        "# Embed to FAISS database (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0SKx0L-QyH4"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define lists to save to FAISS later\n",
        "all_features = []\n",
        "all_labels = []\n",
        "all_biomarkers = []\n",
        "\n",
        "# Here, we want to run inference and extract the learned features/embeddings.\n",
        "with torch.no_grad():\n",
        "  # Extract features from our test_dataloader dataset\n",
        "    for batch in tqdm(train_dataloader, desc=\"Extracting features @ training\"):\n",
        "        images = batch['image pixel values'].to(device)\n",
        "        labels = batch['label'].long().to(device)\n",
        "        biomarkers = batch['biomarkers'].float().to(device)\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Extract features just before disease head\n",
        "        _, _, features = model(images, return_features=True)  # shape: [B, 256]\n",
        "        features = features.cpu().numpy().astype(\"float32\")   # FAISS expects float32\n",
        "\n",
        "        all_features.append(features) # Features per sample\n",
        "        all_labels.extend(labels.cpu().numpy()) # Labels per sample\n",
        "        all_biomarkers.extend(biomarkers.cpu().numpy())  # Biomarkers per sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP20WDCWTxmz",
        "outputId": "de457bce-68e1-4a10-b0fb-1b17839d232a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(14247, 256)\n"
          ]
        }
      ],
      "source": [
        "features_matrix = np.vstack(all_features)\n",
        "labels_array = np.array(all_labels)\n",
        "biomarkers_array = np.array(all_biomarkers)\n",
        "\n",
        "print(features_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2JEKgD9rBLq"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def normalize_vectors(vectors):\n",
        "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "    return vectors / norms\n",
        "\n",
        "# Normalize your feature matrix\n",
        "features_matrix = normalize_vectors(features_matrix)\n",
        "\n",
        "d = features_matrix.shape[1]  # Feature size (2048)\n",
        "\n",
        "# Build FAISS index with L2 distance\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(features_matrix)\n",
        "\n",
        "save_date = \"Jun_23\"\n",
        "save_dir = f\"/content/drive/MyDrive/Indicium/FAISS_indexes/{save_date}_Jun23-model-finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save index\n",
        "faiss.write_index(index, f\"{save_dir}/oct_features.index\")\n",
        "\n",
        "# Save labels (for ID mapping)\n",
        "np.save(f\"{save_dir}/oct_labels.npy\", labels_array)\n",
        "np.save(f\"{save_dir}/oct_biomarkers.npy\", biomarkers_array)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpvZwC6mM-i"
      },
      "source": [
        "# Load FAISS index from disk and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ye521IWZeLSh"
      },
      "outputs": [],
      "source": [
        "labels_array = np.load(f\"{save_dir}/oct_labels.npy\")\n",
        "biomarkers_array = np.load(f\"{save_dir}/oct_biomarkers.npy\")\n",
        "index = faiss.read_index(f\"{save_dir}/oct_features.index\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sF9ETpXnbgL"
      },
      "source": [
        "# Make a FAISS query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAMji7ovUnCO",
        "outputId": "fd132233-02bb-4739-ce50-cac045ccd586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random example index: 1130\n",
            "\n",
            "Query Results for True Label: 1\n",
            "\n",
            "[0.5096, 0.4903, 0.4885, 0.4733, 0.4887, 0.5045, 0.5041, 0.4905, 0.5095, 0.4744, 0.4992, 0.5151, 0.5005, 0.5243, 0.4894, 0.4963] \n",
            "\n",
            "Rank 1:\n",
            "  Index:       10080\n",
            "  Distance:    1.1553\n",
            "  Label:       1\n",
            "  Biomarkers:  [0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Rank 2:\n",
            "  Index:       5251\n",
            "  Distance:    1.1700\n",
            "  Label:       0\n",
            "  Biomarkers:  [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Rank 3:\n",
            "  Index:       12032\n",
            "  Distance:    1.1701\n",
            "  Label:       0\n",
            "  Biomarkers:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Rank 4:\n",
            "  Index:       920\n",
            "  Distance:    1.1775\n",
            "  Label:       0\n",
            "  Biomarkers:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n",
            "Rank 5:\n",
            "  Index:       10865\n",
            "  Distance:    1.1824\n",
            "  Label:       1\n",
            "  Biomarkers:  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "label_map = {0: \"DR\", 1: \"DME\"}\n",
        "biomarker_fields = [\n",
        "    \"Atrophy / thinning of retinal layers\",\n",
        "    \"Disruption of EZ\",\n",
        "    \"DRIL\",\n",
        "    \"IR hemorrhages\",\n",
        "    \"IR HRF\",\n",
        "    \"Partially attached vitreous face\",\n",
        "    \"Fully attached vitreous face\",\n",
        "    \"Preretinal tissue/hemorrhage\",\n",
        "    \"Vitreous debris\",\n",
        "    \"VMT\",\n",
        "    \"DRT/ME\",\n",
        "    \"Fluid (IRF)\",\n",
        "    \"Fluid (SRF)\",\n",
        "    \"Disruption of RPE\",\n",
        "    \"PED (serous)\",\n",
        "    \"SHRM\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Query the image\n",
        "def get_biomarkers_and_features(model, img):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        biomarkers, _, feat = model(img.to(device), return_features=True)\n",
        "    return biomarkers.cpu(), feat.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "\n",
        "\n",
        "# Directly access the dataset\n",
        "sample_dataset = test_dataloader.dataset\n",
        "\n",
        "# Sample random index\n",
        "random_idx = random.randint(0, len(sample_dataset) - 1)\n",
        "print(f\"Random example index: {random_idx}\")\n",
        "\n",
        "# Get a single example from dataset\n",
        "example = sample_dataset[random_idx]\n",
        "query_img = example['image pixel values'].unsqueeze(0).to(device)\n",
        "true_label = torch.tensor(example['label']).long().unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "# Get query biomarkers and normalized features\n",
        "query_biomarker_logits, query_feature = get_biomarkers_and_features(model, query_img)\n",
        "query_biomarkers = torch.sigmoid(query_biomarker_logits).squeeze().cpu().numpy()\n",
        "rounded_probs = np.round(query_biomarkers, 4).tolist()\n",
        "\n",
        "# Search FAISS, use k=5\n",
        "D, I = index.search(query_feature, k=5)\n",
        "topk_labels = labels_array[I[0]]\n",
        "topk_biomarkers = biomarkers_array[I[0]]\n",
        "topk_distances = D[0]\n",
        "\n",
        "# Pretty print results\n",
        "print(f\"\\nQuery Results for True Label: {true_label.item()}\\n\")\n",
        "print(\"[\" + \", \".join(f\"{x:.4f}\" for x in rounded_probs) + \"] \\n\")\n",
        "\n",
        "for rank, (idx, dist, label, biomarkers) in enumerate(zip(I[0], topk_distances, topk_labels, topk_biomarkers), 1):\n",
        "    print(f\"Rank {rank}:\")\n",
        "    print(f\"  Index:       {idx}\")\n",
        "    print(f\"  Distance:    {dist:.4f}\")\n",
        "    print(f\"  Label:       {label}\")\n",
        "    print(f\"  Biomarkers:  {biomarkers}\")\n",
        "    print(\"-\" * 40)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PVKQMv7n5RXC",
        "F82eGeoryGep",
        "TCB65s9W_HYg",
        "DuPSiJln20kX"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}